{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerDeepSVDD\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.3s\n",
      "Pretraining Autoencoder... Epoch: 0, Loss: 155.672\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 1, Loss: 122.059\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 2, Loss: 97.346\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 3, Loss: 82.185\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 4, Loss: 72.348\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 5, Loss: 65.548\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 6, Loss: 60.703\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 7, Loss: 56.948\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 8, Loss: 54.100\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 9, Loss: 51.867\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 10, Loss: 50.048\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 11, Loss: 48.523\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 12, Loss: 47.241\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 13, Loss: 46.129\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 14, Loss: 45.162\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 15, Loss: 44.264\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 16, Loss: 43.460\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 17, Loss: 42.729\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 18, Loss: 42.028\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 19, Loss: 41.375\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 20, Loss: 40.776\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 21, Loss: 40.200\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 22, Loss: 39.635\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 23, Loss: 39.120\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 24, Loss: 38.623\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 25, Loss: 38.179\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 26, Loss: 37.741\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 27, Loss: 37.336\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 28, Loss: 36.970\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 29, Loss: 36.604\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 30, Loss: 36.269\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 31, Loss: 35.896\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 32, Loss: 35.561\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 33, Loss: 35.210\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 34, Loss: 34.865\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 35, Loss: 34.528\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 36, Loss: 34.198\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 37, Loss: 33.844\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 38, Loss: 33.492\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 39, Loss: 33.109\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 40, Loss: 32.721\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 41, Loss: 32.308\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 42, Loss: 31.871\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 43, Loss: 31.424\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 44, Loss: 30.934\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 45, Loss: 30.407\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 46, Loss: 29.866\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 47, Loss: 29.328\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 48, Loss: 28.755\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 49, Loss: 28.178\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 50, Loss: 27.813\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 51, Loss: 27.756\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 52, Loss: 27.694\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 53, Loss: 27.629\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 54, Loss: 27.574\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 55, Loss: 27.519\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 56, Loss: 27.448\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 57, Loss: 27.398\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 58, Loss: 27.330\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 59, Loss: 27.277\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 60, Loss: 27.219\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 61, Loss: 27.165\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 62, Loss: 27.102\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 63, Loss: 27.045\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 64, Loss: 26.980\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 65, Loss: 26.923\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 66, Loss: 26.863\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 67, Loss: 26.794\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 68, Loss: 26.738\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 69, Loss: 26.672\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 70, Loss: 26.617\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 71, Loss: 26.567\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 72, Loss: 26.491\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 73, Loss: 26.434\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 74, Loss: 26.367\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 75, Loss: 26.312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 76, Loss: 26.253\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 77, Loss: 26.181\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 78, Loss: 26.125\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 79, Loss: 26.051\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 80, Loss: 25.988\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 81, Loss: 25.925\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 82, Loss: 25.874\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 83, Loss: 25.800\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 84, Loss: 25.724\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 85, Loss: 25.671\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 86, Loss: 25.618\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 87, Loss: 25.545\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 88, Loss: 25.479\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 89, Loss: 25.404\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 90, Loss: 25.327\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 91, Loss: 25.287\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 92, Loss: 25.207\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 93, Loss: 25.148\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 94, Loss: 25.081\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 95, Loss: 25.015\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 96, Loss: 24.941\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 97, Loss: 24.875\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 98, Loss: 24.801\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 99, Loss: 24.753\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 100, Loss: 24.664\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 101, Loss: 24.584\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 102, Loss: 24.531\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 103, Loss: 24.456\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 104, Loss: 24.396\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 105, Loss: 24.313\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 106, Loss: 24.232\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 107, Loss: 24.169\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 108, Loss: 24.091\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 109, Loss: 24.025\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 110, Loss: 23.950\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 111, Loss: 23.884\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 112, Loss: 23.808\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 113, Loss: 23.718\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 114, Loss: 23.651\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 115, Loss: 23.569\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 116, Loss: 23.489\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 117, Loss: 23.431\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 118, Loss: 23.352\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 119, Loss: 23.273\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 120, Loss: 23.195\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 121, Loss: 23.125\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 122, Loss: 23.048\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 123, Loss: 22.972\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 124, Loss: 22.899\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 125, Loss: 22.814\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 126, Loss: 22.748\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 127, Loss: 22.669\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 128, Loss: 22.590\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 129, Loss: 22.518\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 130, Loss: 22.437\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 131, Loss: 22.367\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 132, Loss: 22.287\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 133, Loss: 22.211\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 134, Loss: 22.149\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 135, Loss: 22.062\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 136, Loss: 21.986\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 137, Loss: 21.924\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 138, Loss: 21.843\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 139, Loss: 21.778\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 140, Loss: 21.712\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 141, Loss: 21.625\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 142, Loss: 21.557\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 143, Loss: 21.497\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 144, Loss: 21.422\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 145, Loss: 21.353\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 146, Loss: 21.269\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 147, Loss: 21.203\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 148, Loss: 21.128\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 149, Loss: 21.053\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "\n",
    "    num_epochs=150\n",
    "    num_epochs_ae=150\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    weight_decay=0.5e-6\n",
    "    weight_decay_ae=0.5e-3\n",
    "    lr_ae=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=200\n",
    "    pretrain=True\n",
    "    latent_dim=True\n",
    "    normal_class=3\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_mnist(args)\n",
    "\n",
    "deep_SVDD = TrainerDeepSVDD(args, data, device)\n",
    "\n",
    "if args.pretrain:\n",
    "    deep_SVDD.pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 0, Loss: 0.442\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 1, Loss: 0.186\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 2, Loss: 0.098\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.062\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.045\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.035\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.029\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.024\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.021\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.018\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.017\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.015\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.014\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.013\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.011\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.010\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.010\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.010\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.009\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.008\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.008\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.008\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.007\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.007\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.007\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.007\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 50, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 51, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 52, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 53, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 54, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 55, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 56, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 57, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 58, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 59, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 60, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 61, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 62, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 63, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 64, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 65, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 66, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 67, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 68, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 69, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 70, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 71, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 72, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 73, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 74, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 75, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 76, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 77, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 78, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 79, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 80, Loss: 0.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 81, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 82, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 83, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 84, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 85, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 86, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 87, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 88, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 89, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 90, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 91, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 92, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 93, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 94, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 95, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 96, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 97, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 98, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 99, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 100, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 101, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 102, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 103, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 104, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 105, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 106, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 107, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 108, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 109, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 110, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 111, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 112, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 113, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 114, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 115, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 116, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 117, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 118, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 119, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 120, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 121, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 122, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 123, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 124, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 125, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 126, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 127, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 128, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 129, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 130, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 131, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 132, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 133, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 134, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 135, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 136, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 137, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 138, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 139, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 140, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 141, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 142, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 143, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 144, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 145, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 146, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 147, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 148, Loss: 0.002\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 149, Loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "deep_SVDD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "ROC AUC score: 0.851\n"
     ]
    }
   ],
   "source": [
    "from test import eval\n",
    "\n",
    "labels, scores = eval(deep_SVDD.net, deep_SVDD.c, data[1], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "plt.xlim(-0.05, 0.08)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
