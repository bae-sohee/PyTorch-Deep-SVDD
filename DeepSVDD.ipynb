{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerDeepSVDD\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.3ss\n",
      "Pretraining Autoencoder... Epoch: 0, Loss: 144.933\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 1, Loss: 112.835\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 2, Loss: 92.621\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 3, Loss: 79.657\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 4, Loss: 70.741\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 5, Loss: 64.212\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 6, Loss: 59.539\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 7, Loss: 55.931\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 8, Loss: 53.061\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 9, Loss: 50.813\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 10, Loss: 49.010\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 11, Loss: 47.519\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 12, Loss: 46.266\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 13, Loss: 45.165\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 14, Loss: 44.134\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 15, Loss: 43.207\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 16, Loss: 42.348\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 17, Loss: 41.575\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 18, Loss: 40.818\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 19, Loss: 40.188\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 20, Loss: 39.561\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 21, Loss: 39.011\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 22, Loss: 38.499\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 23, Loss: 37.993\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 24, Loss: 37.508\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 25, Loss: 37.093\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 26, Loss: 36.703\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 27, Loss: 36.312\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 28, Loss: 35.956\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 29, Loss: 35.636\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 30, Loss: 35.303\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 31, Loss: 35.016\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 32, Loss: 34.716\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 33, Loss: 34.454\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 34, Loss: 34.177\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 35, Loss: 33.920\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 36, Loss: 33.692\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 37, Loss: 33.463\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 38, Loss: 33.237\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 39, Loss: 33.021\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 40, Loss: 32.819\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 41, Loss: 32.620\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 42, Loss: 32.405\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 43, Loss: 32.221\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 44, Loss: 32.009\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 45, Loss: 31.810\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 46, Loss: 31.620\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 47, Loss: 31.429\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 48, Loss: 31.230\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 49, Loss: 31.044\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 50, Loss: 30.916\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 51, Loss: 30.895\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 52, Loss: 30.891\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 53, Loss: 30.858\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 54, Loss: 30.838\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 55, Loss: 30.808\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 56, Loss: 30.796\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 57, Loss: 30.777\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 58, Loss: 30.756\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 59, Loss: 30.732\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 60, Loss: 30.731\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 61, Loss: 30.694\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 62, Loss: 30.658\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 63, Loss: 30.644\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 64, Loss: 30.631\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 65, Loss: 30.600\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 66, Loss: 30.593\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 67, Loss: 30.544\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 68, Loss: 30.529\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 69, Loss: 30.505\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 70, Loss: 30.506\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 71, Loss: 30.467\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 72, Loss: 30.432\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 73, Loss: 30.415\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 74, Loss: 30.385\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 75, Loss: 30.360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 76, Loss: 30.338\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 77, Loss: 30.329\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 78, Loss: 30.280\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 79, Loss: 30.256\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 80, Loss: 30.238\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 81, Loss: 30.203\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 82, Loss: 30.171\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 83, Loss: 30.145\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 84, Loss: 30.127\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 85, Loss: 30.099\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 86, Loss: 30.058\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 87, Loss: 30.035\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 88, Loss: 29.996\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 89, Loss: 29.971\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 90, Loss: 29.946\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 91, Loss: 29.913\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 92, Loss: 29.892\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 93, Loss: 29.855\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 94, Loss: 29.830\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 95, Loss: 29.798\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 96, Loss: 29.753\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 97, Loss: 29.737\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 98, Loss: 29.710\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 99, Loss: 29.661\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 100, Loss: 29.639\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 101, Loss: 29.598\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 102, Loss: 29.572\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 103, Loss: 29.538\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 104, Loss: 29.503\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 105, Loss: 29.481\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 106, Loss: 29.440\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 107, Loss: 29.389\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 108, Loss: 29.355\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 109, Loss: 29.325\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 110, Loss: 29.291\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 111, Loss: 29.256\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 112, Loss: 29.222\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 113, Loss: 29.191\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 114, Loss: 29.145\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 115, Loss: 29.099\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 116, Loss: 29.052\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 117, Loss: 29.032\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 118, Loss: 28.980\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 119, Loss: 28.940\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 120, Loss: 28.911\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 121, Loss: 28.858\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 122, Loss: 28.827\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 123, Loss: 28.788\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 124, Loss: 28.741\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 125, Loss: 28.699\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 126, Loss: 28.650\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 127, Loss: 28.617\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 128, Loss: 28.552\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 129, Loss: 28.521\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 130, Loss: 28.478\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 131, Loss: 28.437\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 132, Loss: 28.385\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 133, Loss: 28.343\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 134, Loss: 28.288\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 135, Loss: 28.240\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 136, Loss: 28.204\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 137, Loss: 28.155\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 138, Loss: 28.104\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 139, Loss: 28.048\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 140, Loss: 27.989\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 141, Loss: 27.952\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 142, Loss: 27.898\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 143, Loss: 27.836\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 144, Loss: 27.784\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 145, Loss: 27.740\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 146, Loss: 27.692\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 147, Loss: 27.637\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 148, Loss: 27.579\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 149, Loss: 27.530\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "\n",
    "    num_epochs=150\n",
    "    num_epochs_ae=150\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    weight_decay=0.5e-6\n",
    "    weight_decay_ae=0.5e-3\n",
    "    lr_ae=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=200\n",
    "    pretrain=True\n",
    "    latent_dim=True\n",
    "    normal_class=3\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_mnist(args)\n",
    "\n",
    "deep_SVDD = TrainerDeepSVDD(args, data, device)\n",
    "\n",
    "if args.pretrain:\n",
    "    deep_SVDD.pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 0, Loss: 0.905\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 1, Loss: 0.412\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 2, Loss: 0.214\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.125\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.082\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.060\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.047\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.040\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.034\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.029\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.026\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.023\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.021\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.019\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.018\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.016\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.015\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.014\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.013\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.012\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.012\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.011\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.011\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.010\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.010\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.009\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.009\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.008\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.008\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.008\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.007\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.007\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.007\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.006\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.005\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 50, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 51, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 52, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 53, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 54, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 55, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 56, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 57, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 58, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 59, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 60, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 61, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 62, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 63, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 64, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 65, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 66, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 67, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 68, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 69, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 70, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 71, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 72, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 73, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 74, Loss: 0.004\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 75, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 76, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 77, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 78, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 79, Loss: 0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 80, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 81, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 82, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 83, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 84, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 85, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 86, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 87, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 88, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 89, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 90, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 91, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 92, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 93, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 94, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 95, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 96, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 97, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 98, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 99, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 100, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 101, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 102, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 103, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 104, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 105, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 106, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 107, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 108, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 109, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 110, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 111, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 112, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 113, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 114, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 115, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 116, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 117, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 118, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 119, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 120, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 121, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 122, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 123, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 124, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 125, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 126, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 127, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 128, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 129, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 130, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 131, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 132, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 133, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 134, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 135, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 136, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 137, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 138, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 139, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 140, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 141, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 142, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 143, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 144, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 145, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 146, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 147, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 148, Loss: 0.003\n",
      "6131/6131: [==============================>.] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 149, Loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "deep_SVDD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = deep_SVDD.net\n",
    "c = deep_SVDD.c\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in deep_SVDD.test_loader:\n",
    "        x = x.float().to(device)\n",
    "        z = net(x)\n",
    "        dist = torch.sum((z - c) ** 2, dim=1)\n",
    "        scores.append(dist.detach().cpu())\n",
    "        labels.append(y.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(scores).numpy()\n",
    "labels = torch.cat(labels).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "scores_in = scores[np.where(labels==args.normal_class)[0]]\n",
    "scores_out = scores[np.where(labels!=args.normal_class)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "plt.xlim(-0.05, 0.08)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.where(labels!=args.normal_class)[0]] = 20\n",
    "labels[np.where(labels==args.normal_class)[0]] = 10\n",
    "labels[np.where(labels==10)[0]] = 0\n",
    "labels[np.where(labels==20)[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8381346710866859"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "test_auc = roc_auc_score(labels, scores)\n",
    "test_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
